{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env_load')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path=os.getenv('csv_file_path')  # Provide the correct path to your CSV file for loading data\n",
    "db_name=os.getenv('db_name')\n",
    "db_user=os.getenv('db_user')\n",
    "db_password=os.getenv('db_password')\n",
    "db_host=os.getenv('db_host') \n",
    "db_port = os.getenv('db_port') \n",
    "schema_name=os.getenv('schema_name')\n",
    "table_name='customers_table'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed for datatypes registration otherwise error\n",
    "see https://stackoverflow.com/questions/39564755/programmingerror-psycopg2-programmingerror-cant-adapt-type-numpy-ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "\n",
    "def addapt_numpy_float64(numpy_float64):\n",
    "    return AsIs(numpy_float64)\n",
    "\n",
    "def addapt_numpy_int64(numpy_int64):\n",
    "    return AsIs(numpy_int64)\n",
    "\n",
    "def addapt_numpy_float32(numpy_float32):\n",
    "    return AsIs(numpy_float32)\n",
    "\n",
    "def addapt_numpy_int32(numpy_int32):\n",
    "    return AsIs(numpy_int32)\n",
    "\n",
    "def addapt_numpy_array(numpy_array):\n",
    "    return AsIs(tuple(numpy_array))\n",
    "\n",
    "register_adapter(np.float64, addapt_numpy_float64)\n",
    "register_adapter(np.int64, addapt_numpy_int64)\n",
    "register_adapter(np.float32, addapt_numpy_float32)\n",
    "register_adapter(np.int32, addapt_numpy_int32)\n",
    "register_adapter(np.ndarray, addapt_numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_postgres(csv_file_path, db_name, db_user, db_password, db_host, db_port, schema_name, table_name):\n",
    "    \"\"\"Function to load data from csv to postgres\"\"\"\n",
    "    # Step 1: Read the CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # for d_c in df.columns:\n",
    "    #     if df[d_c].dtype.name == 'int64':\n",
    "    #         df[d_c] = df[d_c].astype('int32')\n",
    "\n",
    "    # convert data types\n",
    "    df = df.convert_dtypes()\n",
    "    # Step 2: Connect to PostgreSQL Database\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user=db_user,\n",
    "            password=db_password,\n",
    "            host=db_host,\n",
    "            port=db_port\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        print(\"Successfully connected to the database!\")\n",
    "        # create schema if not exists\n",
    "        create_schema_query = f\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS {schema_name}\n",
    "        \"\"\"\n",
    "        # drop old table if exists\n",
    "        drop_table_query = f\"\"\"\n",
    "        DROP TABLE IF EXISTS {schema_name}.{table_name} CASCADE\n",
    "        \"\"\"\n",
    "        # Step 3: Create the table if it doesn't exist\n",
    "        create_table_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n",
    "            customer_id int8 NOT NULL PRIMARY KEY,\n",
    "            \"name\" varchar(128) NULL,\n",
    "            address varchar(128) NULL,\n",
    "            zip_code varchar(100) NULL,\n",
    "            credit_rating int8 NULL,\n",
    "            age int8 NULL,\n",
    "            gender varchar(50) NULL,\n",
    "            marital_status varchar(50) NULL,\n",
    "            profession varchar(100) NULL,\n",
    "            nbr_years_cli int4 NULL,\n",
    "            risk_score float4 NULL,\n",
    "            state varchar(100) NULL,\n",
    "            city varchar(100) NULL,\n",
    "            profile_url varchar(500) NULL,\n",
    "            ssn varchar(20),\n",
    "            phone_number varchar(30),\n",
    "            email varchar(50)\n",
    "            );\n",
    "        \"\"\".format(table_name)\n",
    "        \n",
    "        cur.execute(create_schema_query)\n",
    "        print(f\"Schema {schema_name} created in the database\")\n",
    "        cur.execute(drop_table_query)\n",
    "        cur.execute(create_table_query)\n",
    "        \n",
    "        print(f\"Table `{table_name}` is ready in the database.\")\n",
    "\n",
    "        # Step 4: Prepare the data for bulk insert using `execute_values()`\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {schema_name}.{table_name} (\n",
    "                CUSTOMER_ID, NAME, ADDRESS, ZIP_CODE, CREDIT_RATING, AGE, \n",
    "                GENDER, MARITAL_STATUS, PROFESSION, NBR_YEARS_CLI, RISK_SCORE, \n",
    "                STATE, CITY, profile_url,SSN, PHONE_NUMBER, EMAIL\n",
    "            ) VALUES %s;\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 5: Convert DataFrame rows into a list of tuples for bulk insert\n",
    "        data_to_insert = [tuple(x) for x in df.to_records(index=False)]\n",
    "\n",
    "        # Step 6: Execute the bulk insert\n",
    "        execute_values(cur, insert_query, data_to_insert)\n",
    "\n",
    "        # Commit changes\n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted into the table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the database!\n",
      "Schema bankdemo created in the database\n",
      "Table `customers_table` is ready in the database.\n",
      "Data successfully inserted into the table.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "load_csv_to_postgres(\n",
    "    csv_file_path=csv_file_path,  \n",
    "    db_name=db_name,\n",
    "    db_user=db_user,\n",
    "    db_password=db_password,\n",
    "    db_host=db_host, \n",
    "    db_port=db_port,       \n",
    "    schema_name=schema_name,\n",
    "    table_name=table_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
