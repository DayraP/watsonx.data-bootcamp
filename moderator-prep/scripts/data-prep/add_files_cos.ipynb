{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6603f062",
   "metadata": {},
   "source": [
    "## Add initial data to COS from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7175a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import requests\n",
    "import xmltodict\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./.env_load')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ef254",
   "metadata": {},
   "source": [
    "## Configurations and supporting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b0966",
   "metadata": {},
   "source": [
    "### Read in environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls\n",
    "identityURL = os.getenv(\"IDENTITY_URL\")\n",
    "# buckets location the same as watsonx.data\n",
    "buckets_location = os.getenv(\"COS_BUCKETS_LOCATION\")\n",
    "bucket_endpoint =  f\"https://s3.{buckets_location}.cloud-object-storage.appdomain.cloud\"\n",
    "\n",
    "print(\"COS endpoint - the same as watsonx.data location\", bucket_endpoint)\n",
    "\n",
    "# local files directories\n",
    "\n",
    "# files to be converted to parquet and uploaded into hive bucket\n",
    "files_directory = '../../data/files'\n",
    "\n",
    "# pdfs to be uploaded into input data bucket\n",
    "pdfs_directory = '../../data/pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddf5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials and configurations\n",
    "cloud_api_key = os.getenv(\"CLOUD_API_KEY\")\n",
    "cos_instance_crn = os.getenv(\"COS_INSTANCE_CRN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffe01a1",
   "metadata": {},
   "source": [
    "### Generate token and use it for the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfaa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token():\n",
    "    \"\"\"To geneate user token for other requests\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        'grant_type': f'urn:ibm:params:oauth:grant-type:apikey',\n",
    "        'apikey': f\"{cloud_api_key}\"\n",
    "    }\n",
    "\n",
    "    res = requests.post(f'{identityURL}', headers=headers, data=payload, verify=False)\n",
    "    if res.status_code in [200, 201, 202]:\n",
    "        print(\"Successfully generated token\")\n",
    "    else:\n",
    "        print(\"Code for token generation\", res.status_code)\n",
    "        print(\"Message\", res.text)\n",
    "    cur_string = res.json()\n",
    "    access_token = cur_string['access_token']\n",
    "\n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adcfe1",
   "metadata": {},
   "source": [
    "### Functions to create sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54167c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cos_session(access_token):\n",
    "    s_cos = requests.Session()\n",
    "    s_cos.headers.clear()\n",
    "    headers_cos={\n",
    "        \"Authorization\":\"Bearer {}\".format(access_token),\n",
    "        \"ibm-service-instance-id\": cos_instance_crn\n",
    "    }\n",
    "\n",
    "    s_cos.headers.update(headers_cos) \n",
    "    print(\"COS API session created\")\n",
    "    return s_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a394e",
   "metadata": {},
   "source": [
    "### Create COS session and find hive and input-data buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f982dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_token = generate_token()\n",
    "s_cos = create_cos_session(cur_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f13c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find input bucket name\n",
    "ex_buckets_names = []\n",
    "input_bucket = \"\"\n",
    "hive_bucket = \"\"\n",
    "\n",
    "r = s_cos.get(bucket_endpoint)\n",
    "buckets_list = xmltodict.parse(r.text)['ListAllMyBucketsResult']['Buckets']\n",
    "\n",
    "for bucket_name in buckets_list[\"Bucket\"]:\n",
    "    if bucket_name['Name'].startswith(\"input-data\"):\n",
    "        input_bucket = bucket_name['Name']\n",
    "    if \"hive\" in bucket_name['Name']:\n",
    "        hive_bucket = bucket_name['Name']\n",
    "if input_bucket == \"\":\n",
    "    print(\"Input bucket was not found, add name manually\")\n",
    "else:\n",
    "    print(f\"Identified input bucket as {input_bucket} in {bucket_endpoint}\")\n",
    "\n",
    "if hive_bucket == \"\":\n",
    "    print(\"Hive bucket was not found, add name manually\")\n",
    "else:\n",
    "    print(f\"Identified hive bucket as {hive_bucket} in {bucket_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f43b3f",
   "metadata": {},
   "source": [
    "## Binary files upload - Hive bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79177ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local directory with files\n",
    "\n",
    "\n",
    "print(f\"Will save data from {files_directory} to\", hive_bucket, \"at endpoint\", bucket_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dec045",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dfs =  dict()\n",
    "files_dfs['accounts'] = pd.read_csv(f'{files_directory}/accounts.csv')\n",
    "files_dfs['holdings_up_2023'] = pd.read_csv(f'{files_directory}/holdings_up_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_cos(df_out: pd.DataFrame, output_path: str, convert_parquet=True) -> None:\n",
    "    \"\"\"\n",
    "    Saves df_out as parquet or json file on output_path in COS\n",
    "    \"\"\"\n",
    "    with io.BytesIO() as output:\n",
    "        if convert_parquet:\n",
    "            df_out.to_parquet(output, index=False)\n",
    "        else:\n",
    "            df_out.to_json(output, orient = 'records', index=False, lines=True)\n",
    "        data_object = output.getvalue()\n",
    "\n",
    "    url_new_file = f\"{bucket_endpoint}/{hive_bucket}/{output_path}\"\n",
    "    r = s_cos.put(url_new_file, data=data_object)\n",
    "    print(\"Status code\", r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14914c9c",
   "metadata": {},
   "source": [
    "### Convert data types and save as parquet in COS `HIVE_BUCKET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_n in files_dfs:\n",
    "\n",
    "    #data types original\n",
    "    display(files_dfs[d_n].head())\n",
    "    display(files_dfs[d_n].info())\n",
    "\n",
    "    if 'customer_id' in files_dfs[d_n].columns:\n",
    "        files_dfs[d_n]['customer_id'] = files_dfs[d_n]['customer_id'].astype('int32')\n",
    "\n",
    "    if 'account_id' in files_dfs[d_n].columns:\n",
    "        files_dfs[d_n]['account_id'] = files_dfs[d_n]['account_id'].astype('int32')\n",
    "\n",
    "    if 'tax_liability' in files_dfs[d_n].columns:\n",
    "        files_dfs[d_n]['tax_liability'] = files_dfs[d_n]['tax_liability'].astype('float')\n",
    "\n",
    "    print('After datatypes conversions')\n",
    "    display(files_dfs[d_n].head())\n",
    "    display(files_dfs[d_n].info())\n",
    "\n",
    "    save_data_cos(files_dfs[d_n], output_path = f'input_data_hive/{d_n}_ht/{d_n}.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c930c11",
   "metadata": {},
   "source": [
    "### Save `tax_liability.json` to COS `HIVE_BUCKET`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_f_name = 'tax_liability'\n",
    "json_path = f'{files_directory}/{json_f_name}.json'\n",
    "cur_df = pd.read_json(json_path, orient=\"records\")\n",
    "save_data_cos(cur_df,output_path = f'input_data_hive/{json_f_name}_ht/{json_f_name}.json', convert_parquet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5463b7",
   "metadata": {},
   "source": [
    "## Pdfs for RAG to input bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdfd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Will be adding data {pdfs_directory} to {input_bucket} in {bucket_endpoint}\")\n",
    "\n",
    "for root, dirs, files in os.walk(pdfs_directory):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".pdf\"):\n",
    "            local_path = os.path.join(root, file)\n",
    "            output_path = os.path.join('pdfs', file).replace(\"\\\\\", \"/\")\n",
    "            url_new_file = f\"{bucket_endpoint}/{input_bucket}/{output_path}\"\n",
    "            print(f\"Uploading {local_path} to {url_new_file}\")\n",
    "            with open(local_path, \"rb\") as b_file:\n",
    "                r = s_cos.put(url_new_file, data=b_file)\n",
    "                print(f\"Status code for {local_path} is {r.status_code}\")\n",
    "                if r.status_code not in [200, 201, 202]:\n",
    "                    print(\"Didn' succeed, response is\", r.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
